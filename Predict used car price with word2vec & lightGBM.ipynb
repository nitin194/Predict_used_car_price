{"cells":[{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\nsns.set_palette('Set2')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Supress Scientific notation in python\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\n# Display all columns of long dataframe\npd.set_option('display.max_columns', None)\n\nimport re\n\nfrom math import sqrt \nfrom sklearn.metrics import mean_squared_log_error\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\n\nimport pandas_profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import datasets\ntrain = pd.read_excel('../input/Data_Train.xlsx')\ntest = pd.read_excel('../input/Data_Test.xlsx')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checkout the shape of datasets\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the different aspects of the dataset through amazing pandas profiling tool"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.profile_report()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define categorical features\ncategorical_feature = ['Name','Location','Fuel_Type','Transmission','Owner_Type']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"+\"There are \" + str(mis_val_table_ren_columns.shape[0]) +\" columns that have missing values.\")        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[categorical_feature].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[categorical_feature].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clear difference in the uniqueness of **Name** and **Fuel_type** feature. Lets check and remove the redundant records from the training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Fuel_Type which is not in test set\nlist(train.Fuel_Type[~train.Fuel_Type.isin(test.Fuel_Type)].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are **no** Electric vehicles in the **Test** set. Hence remove the rows from training set where **Fuel_type** is **Electric**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.Fuel_Type != 'Electric']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Coming to Names column, Lets check the naming pattern of different cars in the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Full_name\"] = train.Name.copy()\ntest[\"Full_name\"] = test.Name.copy()\ntrain.Name.sample(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Crazy naming convention. Lets find unique car brands in the train set which are **not** in test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp1 = list(train.Name.str.split(' ').str[0].unique())\ntemp2 = list(test.Name.str.split(' ').str[0].unique())\ntemp3 = [item for item in temp1 if item not in temp2]\ntemp3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets remove the records which contains these brands from the train set first"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[~train.Name.str.contains('|'.join(temp3))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets remove the year content in the names"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_year(data):\n    result = re.search(\"([0-9]+[-]+[0-9]+)\",data)\n    if result:\n        arr = data.replace(result.group(1),\"\")\n        return arr\n    else:\n        return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Name = train.Name.apply(lambda x: remove_year(x))\ntest.Name = test.Name.apply(lambda x: remove_year(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's remove special characters like **[- . /()]** etc"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_char(str):\n    arr = ' '.join(str.split()) #replace multiple spaces to single space\n    arr = re.sub(r\"[-(){}<>/\\.,]\",\"\", arr) #remove special characters\n    return arr.lower() #lowercase all characters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Full_name = train.Name.apply(lambda x: remove_char(x))\ntest.Full_name = test.Name.apply(lambda x: remove_char(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets keep only the **first two relevant words** in the car names and remove the rest to be consistent across the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Name = train.Full_name.apply(lambda x: \" \".join(x.split(' ')[:2]))\ntest.Name = test.Full_name.apply(lambda x: \" \".join(x.split(' ')[:2]))\n\n# Filter brand name for a more generic aggregation in further calculations\ntrain['brand'] = train.Name.apply(lambda x: \" \".join(x.split(' ')[:1]))\ntest['brand'] = test.Name.apply(lambda x: \" \".join(x.split(' ')[:1]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the car names after processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Name.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Moving on to the other metrics, we will correct the feature values as per below strategy \n\n* Remove the New_price currency unit from each value and convert into relevant amount\n* Remove the Engine capacity unit from each value\n* Remove the Mileage unit and comvert all \"0.0 kmpl\" value to NaN\n* Remove the units of Power and convert \"null bhp\" to NaN\n\n After all this filtering, we will fill the missing values with the mean or median value respective to each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define function to correct the New_Price value\ndef price_correct(x):\n    if str(x).endswith('Lakh'):\n        return float(str(x).split()[0])*100000\n    elif str(x).endswith('Cr'):\n        return float(str(x).split()[0])*10000000\n    else:\n        return x\n\ntrain.New_Price = train.New_Price.apply(price_correct)\ntest.New_Price = test.New_Price.apply(price_correct)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Mileage = train.Mileage.replace('0.0 kmpl', np.NaN).apply(lambda x: str(x).split()[0]).astype(float).round(2) # Convert 0 value to Nan, remove unit and convert to float type and round off to 2 decimal place.\ntrain.Engine = train.Engine.apply(lambda x: str(x).split()[0]).astype(float) # Remove the CC part\ntrain.Power = train.Power.replace('null bhp', np.NaN).apply(lambda x: str(x).split()[0]).astype(float).round(2) # convert null value to NaN than as above\n\ntest.Mileage = test.Mileage.replace('0.0 kmpl', np.NaN).apply(lambda x: str(x).split()[0]).astype(float).round(2)\ntest.Engine = test.Engine.apply(lambda x: str(x).split()[0]).astype(float)\ntest.Power = test.Power.replace('null bhp', np.NaN).apply(lambda x: str(x).split()[0]).astype(float).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets fill the missing values relevant to there Brand and model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill missing values aggregating by Name mean and median\ntrain.Engine = train.groupby('Name').Engine.apply(lambda x: x.fillna(x.median()))\ntrain.Power = train.groupby('Name').Power.apply(lambda x: x.fillna(x.mean()))\ntrain.Mileage = train.groupby('Name').Mileage.apply(lambda x: x.fillna(x.mean()))\ntrain.Seats = train.groupby('Name').Seats.apply(lambda x: x.fillna(x.median()))\ntrain.New_Price = train.groupby('Name').New_Price.apply(lambda x: x.fillna(x.mean()))\n\ntest.Engine = test.groupby('Name').Engine.apply(lambda x: x.fillna(x.median()))\ntest.Power = test.groupby('Name').Power.apply(lambda x: x.fillna(x.mean()))\ntest.Mileage = test.groupby('Name').Mileage.apply(lambda x: x.fillna(x.mean()))\ntest.Seats = test.groupby('Name').Seats.apply(lambda x: x.fillna(x.median()))\ntest.New_Price = test.groupby('Name').New_Price.apply(lambda x: x.fillna(x.mean()))\n\n# Fill remaining missing values aggregating by brand mean and median\ntrain.Power = train.groupby('brand').Power.apply(lambda x: x.fillna(x.mean()))\ntrain.Mileage = train.groupby('brand').Mileage.apply(lambda x: x.fillna(x.mean()))\ntrain.Seats = train.groupby('brand').Seats.apply(lambda x: x.fillna(x.median()))\ntrain.New_Price = train.groupby('brand').New_Price.apply(lambda x: x.fillna(x.mean()))\n\ntest.Power = test.groupby('brand').Power.apply(lambda x: x.fillna(x.mean()))\ntest.New_Price = test.groupby('brand').New_Price.apply(lambda x: x.fillna(x.mean()))\n\n# Fill remaining missing values aggregating by whole column mean\ntrain.New_Price = train.New_Price.fillna(train.New_Price.mean())\ntest.New_Price = test.New_Price.fillna(test.New_Price.mean())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's checkout the missing values again."},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check the record for missing Power in test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test.Power.isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets fill this one value with the mean value of the cars having similar Engine capacity"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.Power.fillna(test[test.Engine.between(1900,2000)].Power.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets check the distribution of each relevant feature"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Define a function to plot the distribution of various features\ndef count_plot(data,col,figx,figy,rotate = 'N', order = 'Y'):\n    plt.figure(figsize=(figx, figy));\n    if order == 'Y':\n        g = sns.countplot(x=col, data=data, order = data[col].value_counts().index)\n    else:\n        g = sns.countplot(x=col, data=data)\n    plt.title('Distribution of %s' %col);\n    if rotate == 'Y':\n        plt.xticks(rotation=45);\n    ax=g.axes\n    for p in ax.patches:\n         ax.annotate(f\"{p.get_height() * 100 / data.shape[0]:.2f}%\",\n                     (p.get_x() + p.get_width() / 2., p.get_height()),\n                     ha='center', \t# horizontal alignment\n                     va='top',\t\t# Vertical alignment\n                     fontsize=10,\t# Fontsize\n                     color='black',\t# Color set\n                     rotation=0,\t# Rotation type\n                     xytext=(0,10),\t# caption position\n                     textcoords='offset points' # Caption placement\n                    ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Check the Year make distribution of the Training data\ncount_plot(train,'Year',20,5,rotate = 'Y', order = 'N')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Year make distribution of the Test data\ncount_plot(test,'Year',20,5,rotate = 'Y', order = 'N')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Fule type distribution of the Training data\ncount_plot(train,'Fuel_Type',6,6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Fule type distribution of the Test data\ncount_plot(test,'Fuel_Type',6,6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Transmission distribution of the Training data\ncount_plot(train,'Transmission',5,8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Transmission distribution of the Test data\ncount_plot(test,'Transmission',5,8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Transmission distribution of the Training data\ncount_plot(train,'Owner_Type',7,8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Transmission distribution of the Test data\ncount_plot(test,'Owner_Type',7,8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Transmission distribution of the Training data\ncount_plot(train,'Location',20,7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Transmission distribution of the Test data\ncount_plot(test,'Location',20,7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Transmission distribution of the Train data\ncount_plot(train,'Seats',10,6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Transmission distribution of the Test data\ncount_plot(test,'Seats',10,6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observed that there are no vehicles with **0** or **9** seats in the test set. Lets check cars with **0,2,9 and 10** seats in the training data and adjust the training data to align with the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.Seats.isin([0,2,9,10])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data seems legit except for only some records with 0 and 9 seats. Lets fill the 0 audi a4 seat with 5 seats and the other vehicle having 9 seats with 10 seats.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[3999,'Seats'] = 5\n\ntrain.Seats[train.Seats == 9] = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets checkout the power, Mileage and Engine distribution across the Training and the test set"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Define function for the next set of graph distributions.\ndef dist_plot(data, col, bins, color, figx, figy, kde = True):\n    plt.figure(figsize=(figx,figy))\n    sns.distplot(data[col].values, bins=bins, color=color, kde_kws={\"shade\": True}, label=\"Low\", kde=kde)\n    plt.title(\"Histogram of %s Distribution\"%col)\n    plt.xlabel('%s'%col, fontsize=12)\n    plt.ylabel('Vehicle Count', fontsize=12)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Power distribution of the Train data\ndist_plot(train,'Power', 50, 'blue', 20, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Power distribution of the Test data\ndist_plot(test,'Power', 50, 'blue', 20, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Power Data distribution is similar. Lets check the Engine distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Power distribution of the Train data\ndist_plot(train,'Engine', 25, 'brown', 20, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Power distribution of the Test data\ndist_plot(test,'Engine', 25, 'brown', 20, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Engine Data seems to be consistent across both the sets. Lets check the Mileage distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Power distribution of the Train data\ndist_plot(train,'Mileage', 50, 'green', 20, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Power distribution of the Test data\ndist_plot(test,'Mileage', 50, 'green', 20, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mileage records are also similarly distributed in both the sets. Lets check Kilometers_Driven data now"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Power distribution of the Training data\ndist_plot(train,'Kilometers_Driven', 50, 'magenta', 20, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Power distribution of the Test data\ndist_plot(test,'Kilometers_Driven', 50, 'magenta', 20, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the data in **training** set is **highly skewed** which is a pointer to outliers. Lets check those black birds"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'Kilometers_Driven'\nfrom scipy import stats\noutliers = train[col][(np.abs(stats.zscore(train[col])) > 3)]\noutliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the same records for these outlier values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.Kilometers_Driven.isin(outliers)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No other ambiguity found except the KM driven values. Let's remove these and check the distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[~train.Kilometers_Driven.isin(outliers)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dist_plot(train,'Kilometers_Driven', 50, 'magenta', 20, 5, kde = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the Kilometers_Driven distribution of the training set looks more meaningful and in-sync with the test set\n\nLet's move on and create new features from the existing ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Record the age of the car\nimport datetime\ntrain['Age'] = datetime.datetime.now().year - train['Year']\ntest['Age'] = datetime.datetime.now().year - test['Year']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Record the number of words in the Full_name of the car\ntrain['Name_length'] = train.Full_name.apply(lambda x: len(str(x).split(' ')))\ntest['Name_length'] = test.Full_name.apply(lambda x: len(str(x).split(' ')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define categorical features\ncategorical_features = ['Location','Fuel_Type','Transmission','Owner_Type','Seats']\n\n# Define function for dummy operation\ndef get_dummies(dataframe,feature_name):\n  dummy = pd.get_dummies(dataframe[feature_name], prefix=feature_name)\n  dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n  return pd.concat([dataframe,dummy], axis = 1)\n\n# Dummify categorical features\nfor i in categorical_features:\n    train = get_dummies(train, i)\n    test = get_dummies(test, i)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define function to aggregate metrics for different features\ndef aggregate_features(data):   \n    \n    aggregate_dict = {  'Age' : ['count'],\n                        'Mileage' : ['sum','max','min','mean','std','median','skew'],\n                        'Power' : ['sum','max','min','mean','std','median','skew'],\n                        'Engine' : ['sum','max','min','mean','std','median','skew']}\n    \n    data_agg = data.groupby(['Name']).agg(aggregate_dict)\n    data_agg.columns = ['_'.join(col).strip() for col in data_agg.columns.values]\n    data_agg.reset_index(inplace=True)    \n    data_agg = pd.merge(data, data_agg, on='Name', how='left')    \n    return data_agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create aggregated features\ntrain = aggregate_features(train)\ntest = aggregate_features(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the missing values after creating the aggregated features"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very less number of values are missing. Let's fill these values with the mean of the respective column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Mileage_skew = train.Mileage_skew.fillna(train.Mileage_skew.mean())\ntrain.Power_skew = train.Power_skew.fillna(train.Power_skew.mean())\ntrain.Engine_skew = train.Engine_skew.fillna(train.Engine_skew.mean())\ntrain.Mileage_std = train.Mileage_std.fillna(train.Mileage_std.mean())\ntrain.Power_std = train.Power_std.fillna(train.Power_std.mean())\ntrain.Engine_std = train.Engine_std.fillna(train.Engine_std.mean())\n\ntest.Mileage_skew = test.Mileage_skew.fillna(test.Mileage_skew.mean())\ntest.Power_skew = test.Power_skew.fillna(test.Power_skew.mean())\ntest.Engine_skew = test.Engine_skew.fillna(test.Engine_skew.mean())\ntest.Mileage_std = test.Mileage_std.fillna(test.Mileage_std.mean())\ntest.Power_std = test.Power_std.fillna(test.Power_std.mean())\ntest.Engine_std = test.Engine_std.fillna(test.Engine_std.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's move ahead and create aggregated vectors for the vehicle name in each record"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nimport multiprocessing\ncores = multiprocessing.cpu_count()\n\n# Define function to add/aggregate embeddings of single token text\ndef word_vector(tokens, size):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0\n    for word in tokens:\n        try:\n            vec += model_w2v.wv[word].reshape((1, size))\n            count += 1\n        except KeyError:  # handling the case where the token is not in vocabulary\n            continue\n    if count != 0:\n        vec /= count\n    return vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_names = pd.concat([train.Full_name, test.Full_name], ignore_index=True)\ntokens = total_names.apply(lambda x: x.split()) # tokenizing text\ntrain_tokens = train.Full_name.apply(lambda x: x.split()) # tokenizing text\ntest_tokens = test.Full_name.apply(lambda x: x.split()) # tokenizing text\ntokens_size = len(tokens)\ntrain_tokens_size = len(train_tokens)\ntest_tokens_size = len(test_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_w2v = gensim.models.Word2Vec(\n            tokens,\n            size=200, # desired no. of features/independent variables\n            window=2, # context window size\n            min_count=2,\n            sg = 1, # 1 for skip-gram model\n            hs = 0, # off heirarchichal softmax\n            negative = 1, # for negative sampling\n            workers= cores-1, # no.of cores\n#             sample=.1,\n            alpha=0.009, \n            min_alpha=0.0009,\n#             seed=0,\n#             hashfxn=hash\n) \n\nmodel_w2v.train(tokens,\n                total_examples= tokens_size,\n                epochs=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordvec_train_array = np.zeros((train_tokens_size, 200)) \nwordvec_test_array = np.zeros((test_tokens_size, 200))\n\nfor i in range(train_tokens_size):\n    wordvec_train_array[i,:] = word_vector(train_tokens[i], 200)\nwordvec_train_df = pd.DataFrame(wordvec_train_array)\n\nfor i in range(test_tokens_size):\n    wordvec_test_array[i,:] = word_vector(test_tokens[i], 200)\nwordvec_test_df = pd.DataFrame(wordvec_test_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordvec_train_df.shape, wordvec_test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_w2v.wv.most_similar(positive=\"hyundai\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Word2Vec vectors are giving appropriate results. That's nice!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define function for multiple aggregation on a dataframe rows\ndef agg_df(df):\n    return pd.DataFrame(\n                        {'Name_sum':df.sum(axis=1),\n                         'Name_mean':df.mean(axis=1),\n                         'Name_std':df.std(axis=1),\n                         'Name_max':df.max(axis=1),\n                         'Name_min':df.min(axis=1),\n                         'Name_median':df.median(axis=1),\n                         'Name_skew':df.skew(axis=1)\n                        }\n                       )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add aggregated features on the Name word2vec vctors.\ntrain = pd.concat([train, agg_df(wordvec_train_df)], axis=1) \ntest = pd.concat([test, agg_df(wordvec_test_df)], axis=1) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the train dataset after feature aggregation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets have a look at the correlation between various relevant features through Heatmap"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"features = ['Year','Kilometers_Driven','Mileage','Engine','Power','Age','Price']\n\n# Through CORRMAT\nfrom mlens.visualization import corrmat\ncorrmat(train[features].corr(), inflate=False)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here we can observe below things\n\n* Power and Engine are highly correlated.\n* Price has good correlation with Engine and Power.\n* We can see that Price is negatively correlated with Mileage, KMs_Driven and Age which is legit\n\nSo, let's see actual distribution as per the above correlation."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Engine and Power \nplt.figure(figsize=(8,6))\nplt.scatter(train.Power, train.Engine, c='blue')\nplt.xlabel('Power(bhp)', fontsize=12)\nplt.ylabel('Engine(cc)', fontsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both the features **Engine** and **Power** are almost **linearly** corelated."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Engine and Mileage \nplt.figure(figsize=(8,6))\nplt.scatter(train.Mileage, train.Engine, c='blue')\nplt.xlabel('Mileage(kmpl)', fontsize=12)\nplt.ylabel('Engine(cc)', fontsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a very legit correlation between **Engine** and **Mileage** i.e. as the Engine capacity increases its mileage decreases"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Power and Price\nplt.figure(figsize=(8,6))\nplt.scatter(train.Power, train.Price, c='red')\nplt.xlabel('Power(bhp)', fontsize=12)\nplt.ylabel('Price(lacs)', fontsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of course, the increase in **Power** comes with the increase in vehicle **price**. But the price factor seems to have **plateaued** after a certain Power range of 250 bhp"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Engine and Price\nplt.figure(figsize=(8,6))\nplt.scatter(train.Engine, train.Price, c='green')\nplt.xlabel('Engine(cc)', fontsize=12)\nplt.ylabel('Price(lacs)', fontsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Engine** also shows similar pattern as of Power when shocased with the **Price**."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Age and Price\nplt.figure(figsize=(8,6))\nplt.scatter(train.Age, train.Price, c='orange')\nplt.xlabel('Age(years)', fontsize=12)\nplt.ylabel('Price(lacs)', fontsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's right!! Aeging vehicles sells for lesser price as compared to the younger vehicles."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take backup before dropping some features\ntrain_backup = train.copy() \ntest_backup = test.copy() \n\n# Drop irrelevant features\ndrop_features = ['Location','Fuel_Type','Transmission','Owner_Type','Seats','Full_name','Name','brand']\nbackup_train = train.drop(drop_features, axis=1, inplace=True)\nbackup_test = test.drop(drop_features, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign values to variables for training and testing\n\nX_train = train.drop(labels=['Price'], axis=1) # Assign all features except Price to X\ny_train = np.log1p(train['Price'].values) # Convert Price to log scale\nX_test = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the train and test set before feeding to the model\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = pd.DataFrame(sc.fit_transform(X_train),columns = X_train.columns)\nX_test = pd.DataFrame(sc.transform(X_test),columns = X_test.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model training and prediction\n\nWe will use **LGB** because it generally **trains** much **faster** as compared to **XGBoost** and mostly gives **equivalent** or **better results**."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_X = X_train.copy()\ntrain_y = y_train.copy()\ntest_X = X_test.copy()\n\n# Define LGBM function\ndef runLGB(train_X, train_y, val_X=None, val_y=None, test_X=None, dep=-1, seed=0, data_leaf=5):\n    params = {}\n    params[\"objective\"] = \"regression\"\n    params['metric'] = 'l2_root'\n    params['boosting'] = 'gbdt'\n#     params[\"max_depth\"] = dep\n#     params[\"num_leaves\"] = 39\n#     params[\"min_data_in_leaf\"] = data_leaf\n    params[\"learning_rate\"] = 0.009\n    params[\"bagging_fraction\"] = 0.75\n    params[\"feature_fraction\"] = 0.75\n    params[\"feature_fraction_seed\"] = seed\n    params[\"bagging_freq\"] = 1\n    params[\"bagging_seed\"] = seed\n#     params[\"lambda_l2\"] = 5\n#     params[\"lambda_l1\"] = 5\n    params[\"silent\"] = True\n    params[\"random_state\"] = seed,\n    num_rounds = 3000\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n\n    if val_y is not None:\n        lgtest = lgb.Dataset(val_X, label=val_y)\n        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest], early_stopping_rounds=50, verbose_eval=100)\n    else:\n        lgtest = lgb.DMatrix(val_X)\n        model = lgb.train(params, lgtrain, num_rounds)\n\n    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n  \n    loss = 0\n    \n    if val_y is not None:\n        loss = sqrt(mean_squared_log_error(np.expm1(val_y), np.expm1(pred_val_y)))\n        return model, loss, pred_test_y\n    else:\n        return model, loss, pred_test_y\n\n## K-FOLD train\n\ncv_scores = [] # array for keeping cv-scores for each fold.\npred_test_full = 0 # array to keep predictions of each fold.\npred_train = np.zeros(train_X.shape[0])\nn_fold = 10\nprint(f\"Building model over {n_fold} folds\\n\")\nkf = KFold(n_splits=n_fold, shuffle=True, random_state=4)\n\nfeature_importance = pd.DataFrame()\nfor fold_n, (dev_index, val_index) in enumerate(kf.split(train_X, train_y)):    \n    dev_X, val_X = train_X.iloc[dev_index,:], train_X.iloc[val_index,:]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n\n    model, loss, pred_t = runLGB(dev_X, dev_y, val_X, val_y, test_X, dep=8, seed=0)\n      \n    pred_test_full += pred_t\n    print(f\"\\n>>>>RMSLE for fold {fold_n+1} is: {loss}<<<<\\n\")\n    cv_scores.append(loss)\n    \n    # feature importance aggregation over n folds\n    fold_importance = pd.DataFrame()\n    fold_importance[\"feature\"] = X_train.columns\n    fold_importance[\"importance\"] = model.feature_importance()\n    fold_importance[\"fold\"] = fold_n + 1\n    feature_importance = pd.concat([feature_importance, fold_importance], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Mean RMSLE score over folds is: {np.mean(cv_scores)}\")\n\n# Aggregate mean prediction over 10 folds.\npred_test_full /= n_fold\npred_test_final = np.expm1(pred_test_full)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot feature importance mean aggregated over 10 folds\nplt.figure(figsize=(20, 20));\nfeature_importance = pd.DataFrame(feature_importance.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False).reset_index())[:50]\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance);\nplt.title('Feature Importance (average over folds)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create submission file.\nPredict_submission = pd.DataFrame(data=pred_test_final, columns=['Price'])\nwriter = pd.ExcelWriter('Output.xlsx', engine='xlsxwriter')\nPredict_submission.to_excel(writer,sheet_name='Sheet1', index=False)\nwriter.save()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So this was it... This kernel is open to suggestions for improvement over this score through any means other than stacking and hyper-parameter tuning. If you have such idea ... please share and we will try to improve on current score.\n\nThanks for reading guys !!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"308.6px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}